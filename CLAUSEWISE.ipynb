{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPWR3O+CJbuFXHcZS62+HE1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# SINGLE-CELL: Full ClauseWise app (flashy UI + multi-feature backend)\n","# Copy-paste into one Colab python cell and run.\n","\n","# 1) Install dependencies\n","!pip install -q flask flask-cors pyngrok pdfplumber python-docx pillow pytesseract requests pdf2image\n","\n","# 2) Full app code (writes templates/static then runs server)\n","import os, io, time, uuid, tempfile\n","from flask import Flask, request, jsonify, render_template, send_file\n","from flask_cors import CORS\n","from pyngrok import ngrok\n","from PIL import Image\n","import pytesseract\n","import pdfplumber\n","import docx\n","import requests\n","from pdf2image import convert_from_path\n","\n","# ---------------- CONFIG ----------------\n","# Hugging Face token (you provided earlier) - keep private in real use.\n","HUGGING_FACE_TOKEN = \"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\"\n","\n","# Ngrok token\n","NGROK_TOKEN = \"31zncZCWYbr5fHKJ1wXdn4A0191_37bhg5ZoKURDu4eEcumWJ\"\n","\n","# Model to use for generative tasks (prompt-based)\n","HF_MODEL = \"google/flan-t5-large\"   # good general-purpose instruction model\n","HF_API_URL = f\"https://api-inference.huggingface.co/models/{HF_MODEL}\"\n","HF_HEADERS = {\"Authorization\": f\"Bearer {HUGGING_FACE_TOKEN}\"}\n","\n","# Upload folder\n","os.makedirs(\"templates\", exist_ok=True)\n","os.makedirs(\"static\", exist_ok=True)\n","os.makedirs(\"uploads\", exist_ok=True)\n","\n","# ---------------- Flask ----------------\n","app = Flask(__name__, template_folder=\"templates\", static_folder=\"static\")\n","CORS(app)\n","\n","# ---------------- Helper: Text extraction ----------------\n","def ocr_image(pil_img):\n","    try:\n","        return pytesseract.image_to_string(pil_img)\n","    except Exception:\n","        return \"\"\n","\n","def extract_text_from_pdf(path):\n","    text_parts = []\n","    try:\n","        with pdfplumber.open(path) as pdf:\n","            for i, page in enumerate(pdf.pages, start=1):\n","                page_text = page.extract_text()\n","                if page_text and page_text.strip():\n","                    text_parts.append(page_text)\n","                else:\n","                    # fallback: render page image and OCR\n","                    try:\n","                        images = convert_from_path(path, first_page=i, last_page=i, dpi=300)\n","                        if images:\n","                            text_parts.append(ocr_image(images[0]))\n","                    except Exception:\n","                        try:\n","                            img = page.to_image(resolution=300).original\n","                            # page.to_image().original returns numpy array; convert:\n","                            pil = Image.fromarray(img) if hasattr(Image, 'fromarray') else None\n","                            if pil:\n","                                text_parts.append(ocr_image(pil))\n","                        except Exception:\n","                            continue\n","    except Exception:\n","        # fallback full conversion\n","        try:\n","            images = convert_from_path(path, dpi=300)\n","            for img in images:\n","                text_parts.append(ocr_image(img))\n","        except Exception:\n","            return \"\"\n","    return \"\\n\".join([p for p in text_parts if p and p.strip()])\n","\n","def extract_text_generic(path):\n","    ext = os.path.splitext(path)[1].lower()\n","    if ext == \".pdf\":\n","        return extract_text_from_pdf(path)\n","    if ext in [\".docx\"]:\n","        try:\n","            doc = docx.Document(path)\n","            return \"\\n\".join([p.text for p in doc.paragraphs if p.text])\n","        except Exception:\n","            return \"\"\n","    if ext in [\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tiff\"]:\n","        try:\n","            img = Image.open(path)\n","            return ocr_image(img)\n","        except Exception:\n","            return \"\"\n","    if ext in [\".txt\", \".md\", \".csv\"]:\n","        try:\n","            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n","                return f.read()\n","        except Exception:\n","            return \"\"\n","    return \"\"\n","\n","# ---------------- Helper: call HF model (instruction prompt) ----------------\n","def call_hf_instruction(prompt, max_tokens=512):\n","    payload = {\"inputs\": prompt, \"options\":{\"wait_for_model\": True}}\n","    try:\n","        r = requests.post(HF_API_URL, headers=HF_HEADERS, json=payload, timeout=120)\n","        if r.status_code != 200:\n","            return {\"error\": f\"Hugging Face API returned {r.status_code}: {r.text}\"}\n","        data = r.json()\n","        # generative response often in list with generated_text\n","        if isinstance(data, list) and data:\n","            first = data[0]\n","            if isinstance(first, dict) and \"generated_text\" in first:\n","                return {\"text\": first[\"generated_text\"]}\n","            # other models may return different keys\n","            # fallback to stringifying\n","            return {\"text\": str(first)}\n","        elif isinstance(data, dict):\n","            if \"error\" in data:\n","                return {\"error\": data[\"error\"]}\n","            # fallback text\n","            for k in (\"generated_text\", \"summary_text\", \"text\"):\n","                if k in data:\n","                    return {\"text\": data[k]}\n","            return {\"text\": str(data)}\n","        else:\n","            return {\"text\": str(data)}\n","    except Exception as e:\n","        return {\"error\": f\"Request failed: {e}\"}\n","\n","# ---------------- Chunking helper ----------------\n","def chunk_text(text, max_chars=3000):\n","    chunks = []\n","    start = 0\n","    n = len(text)\n","    while start < n:\n","        end = min(start + max_chars, n)\n","        if end < n:\n","            cut = text.rfind(\"\\n\", start, end)\n","            if cut <= start:\n","                cut = text.rfind(\" \", start, end)\n","            if cut > start:\n","                end = cut\n","        chunks.append(text[start:end].strip())\n","        start = end\n","    return [c for c in chunks if c]\n","\n","# ---------------- High-level feature implementations (prompt based) ----------------\n","def feature_simplify(text):\n","    prompt = (\n","        \"You are ClauseWise ‚Äî simplify the legal clauses below into short, clear, layman-friendly language. \"\n","        \"Keep the meaning exactly the same. For each clause, output a heading 'Clause X:' and then the simplified text.\\n\\n\"\n","        f\"Document excerpt:\\n{text}\"\n","    )\n","    return call_hf_instruction(prompt)\n","\n","def feature_ner(text):\n","    prompt = (\n","        \"You are ClauseWise. Extract named entities from the text and return JSON with these top-level keys: \"\n","        \"'parties' (list), 'dates' (list), 'money' (list), 'obligations' (list of short obligations), 'terms' (list of legal terms).\\n\\n\"\n","        f\"Text:\\n{text}\\n\\nReturn only valid JSON.\"\n","    )\n","    return call_hf_instruction(prompt)\n","\n","def feature_clause_extract(text):\n","    prompt = (\n","        \"You are ClauseWise. Split the following legal document into numbered clauses. Output a JSON array of objects \"\n","        \"each with keys: 'clause_number' and 'clause_text'. Keep clauses concise.\\n\\n\"\n","        f\"Document:\\n{text}\\n\\nReturn only JSON.\"\n","    )\n","    return call_hf_instruction(prompt)\n","\n","def feature_classify(text):\n","    prompt = (\n","        \"You are ClauseWise. Classify the uploaded legal document into one of: NDA, Lease, Employment Contract, Service Agreement, Other. \"\n","        \"Give a one-line explanation for the classification. Return JSON like: {\\\"type\\\":\\\"NDA\\\",\\\"confidence\\\":0.9,\\\"explanation\\\":\\\"...\\\"}.\\n\\n\"\n","        f\"Document excerpt:\\n{text}\"\n","    )\n","    return call_hf_instruction(prompt)\n","\n","# ---------------- ROUTES ----------------\n","@app.route(\"/\")\n","def index():\n","    return render_template(\"index.html\")\n","\n","@app.route(\"/analyze\", methods=[\"POST\"])\n","def analyze_route():\n","    if \"file\" not in request.files:\n","        return jsonify({\"error\": \"No file provided\"}), 400\n","\n","    f = request.files[\"file\"]\n","    orig_name = f.filename or f\"upload_{uuid.uuid4().hex}\"\n","    safe_name = orig_name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n","    tmp_path = os.path.join(\"uploads\", f\"{uuid.uuid4().hex}_{safe_name}\")\n","    f.save(tmp_path)\n","\n","    # extract text\n","    text = extract_text_generic(tmp_path)\n","    if not text or text.strip() == \"\":\n","        return jsonify({\"error\": \"Could not extract any readable text from the uploaded file.\"}), 400\n","\n","    # feature selection from request.form (default to all)\n","    feature = request.form.get(\"feature\", \"all\")  # \"simplify\", \"ner\", \"clauses\", \"classify\", \"all\"\n","    # chunking & processing\n","    if feature == \"simplify\":\n","        # chunk large docs and summarize/simplify per chunk\n","        chunks = chunk_text(text, 2500)\n","        out_parts = []\n","        for c in chunks:\n","            res = feature_simplify(c)\n","            if \"error\" in res: return jsonify({\"error\": res[\"error\"]}), 502\n","            out_parts.append(res.get(\"text\",\"\"))\n","        return jsonify({\"result\": \"\\n\\n---\\n\\n\".join(out_parts)})\n","    elif feature == \"ner\":\n","        res = feature_ner(text)\n","        if \"error\" in res: return jsonify({\"error\": res[\"error\"]}), 502\n","        # Try to parse JSON (model should return JSON)\n","        return jsonify({\"result_raw\": res.get(\"text\",\"\")})\n","    elif feature == \"clauses\":\n","        res = feature_clause_extract(text)\n","        if \"error\" in res: return jsonify({\"error\": res[\"error\"]}), 502\n","        return jsonify({\"result_raw\": res.get(\"text\",\"\")})\n","    elif feature == \"classify\":\n","        res = feature_classify(text)\n","        if \"error\" in res: return jsonify({\"error\": res[\"error\"]}), 502\n","        return jsonify({\"result_raw\": res.get(\"text\",\"\")})\n","    else:\n","        # \"all\" - run all features (may take longer)\n","        results = {}\n","        # Simplify (quick chunked)\n","        chunks = chunk_text(text, 2500)\n","        simp_parts = []\n","        for c in chunks:\n","            r = feature_simplify(c)\n","            if \"error\" in r: results[\"simplify_error\"] = r[\"error\"]; break\n","            simp_parts.append(r.get(\"text\",\"\"))\n","        results[\"simplified\"] = \"\\n\\n---\\n\\n\".join(simp_parts)\n","        # NER\n","        ner_r = feature_ner(text)\n","        results[\"ner_raw\"] = ner_r.get(\"text\",\"\") if \"text\" in ner_r else ner_r.get(\"error\", \"\")\n","        # Clauses\n","        cl_r = feature_clause_extract(text)\n","        results[\"clauses_raw\"] = cl_r.get(\"text\",\"\") if \"text\" in cl_r else cl_r.get(\"error\",\"\")\n","        # Classification\n","        clz_r = feature_classify(text)\n","        results[\"classification_raw\"] = clz_r.get(\"text\",\"\") if \"text\" in clz_r else clz_r.get(\"error\",\"\")\n","        return jsonify({\"analysis\": results, \"meta\": {\"chars\": len(text)}})\n","\n","# ---------------- Download report (simple txt) ----------------\n","@app.route(\"/download\", methods=[\"POST\"])\n","def download_report():\n","    payload = request.get_json()\n","    if not payload or \"analysis\" not in payload:\n","        return jsonify({\"error\":\"No analysis provided\"}), 400\n","    fname = payload.get(\"filename\",\"clausewise_report.txt\")\n","    body = []\n","    a = payload[\"analysis\"]\n","    if isinstance(a, dict):\n","        for k,v in a.items():\n","            body.append(f\"=== {k} ===\\n{v}\\n\\n\")\n","    else:\n","        body.append(str(a))\n","    tmp = os.path.join(\"uploads\", f\"report_{uuid.uuid4().hex}.txt\")\n","    with open(tmp, \"w\", encoding=\"utf-8\") as fh:\n","        fh.write(\"\\n\".join(body))\n","    return send_file(tmp, as_attachment=True, download_name=fname)\n","\n","# ------------------ Frontend: Big flashy UI (templates/index.html + static/script.js + static/styles.css) ----------------\n","index_html = r'''\n","<!doctype html>\n","<html lang=\"en\">\n","<head>\n","  <meta charset=\"utf-8\">\n","  <title>ClauseWise ‚Äî Legal Document Analyzer</title>\n","  <meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" />\n","  <link rel=\"stylesheet\" href=\"/static/styles.css\">\n","</head>\n","<body>\n","  <div class=\"app\">\n","    <aside class=\"sidebar\">\n","      <div class=\"brand\"><div class=\"logo\">‚öñÔ∏è</div><div><h2>ClauseWise</h2><div class=\"tag\">Legal Document Analyzer</div></div></div>\n","      <div class=\"desc\">AI-powered clause simplification, NER, clause extraction, classification & more.</div>\n","      <div class=\"controls\">\n","        <label for=\"feature\">Choose feature</label>\n","        <select id=\"feature\">\n","          <option value=\"all\" selected>All (full analysis)</option>\n","          <option value=\"simplify\">Clause Simplification</option>\n","          <option value=\"ner\">Named Entity Recognition (NER)</option>\n","          <option value=\"clauses\">Clause Extraction</option>\n","          <option value=\"classify\">Document Classification</option>\n","        </select>\n","      </div>\n","      <div class=\"footer\">Powered by Granite (placeholder) ‚Ä¢ IBM Watson (placeholder) ‚Ä¢ HF</div>\n","    </aside>\n","\n","    <main class=\"main\">\n","      <header><h1>ClauseWise</h1><p>Upload PDF, DOCX, TXT or Image ‚Äî we extract, analyze & summarize.</p></header>\n","\n","      <section class=\"uploader\">\n","        <div id=\"dropzone\" class=\"dropzone\">\n","          <div class=\"dz-text\">Drop files here or click to browse</div>\n","          <input id=\"fileInput\" type=\"file\" accept=\".pdf,.docx,.txt,.md,.png,.jpg,.jpeg\"/>\n","        </div>\n","        <div class=\"buttons\">\n","          <button id=\"analyzeBtn\" class=\"primary\">Analyze Document</button>\n","          <button id=\"clearBtn\" class=\"ghost\">Clear</button>\n","          <button id=\"downloadBtn\" class=\"small\">Download Report</button>\n","        </div>\n","        <div id=\"progress\" class=\"progress\"><div id=\"bar\" class=\"bar\"></div><div id=\"ptext\" class=\"ptext\">Waiting</div></div>\n","      </section>\n","\n","      <section id=\"results\" class=\"results hidden\">\n","        <div class=\"result-card\">\n","          <div class=\"result-header\"><h3>Analysis Results</h3></div>\n","          <div class=\"grid\">\n","            <div class=\"panel\"><h4>General Summary / Simplified</h4><pre id=\"simplified\"></pre></div>\n","            <div class=\"panel\"><h4>Named Entities (NER)</h4><pre id=\"ner\"></pre></div>\n","            <div class=\"panel full\"><h4>Clauses</h4><pre id=\"clauses\"></pre></div>\n","            <div class=\"panel\"><h4>Classification</h4><pre id=\"classification\"></pre></div>\n","          </div>\n","        </div>\n","      </section>\n","    </main>\n","  </div>\n","  <script src=\"/static/script.js\"></script>\n","</body>\n","</html>\n","'''\n","\n","styles_css = r'''\n",":root{\n","  --bg:#071124;--card:rgba(255,255,255,0.03);--accent:#00d4ff;--muted:#9fb4c8;\n","}\n","*{box-sizing:border-box}\n","html,body{height:100%;margin:0;font-family:Inter,Segoe UI,Roboto;background:linear-gradient(135deg,var(--bg),#0d1b2a);color:#e8f6ff}\n",".app{display:flex;gap:20px;height:100vh;padding:28px}\n",".sidebar{width:300px;background:linear-gradient(180deg,rgba(255,255,255,0.02),rgba(255,255,255,0.01));padding:20px;border-radius:14px;display:flex;flex-direction:column;gap:12px}\n",".brand{display:flex;gap:12px;align-items:center}\n",".logo{width:56px;height:56px;border-radius:12px;background:linear-gradient(90deg,var(--accent),#6b7bff);display:flex;align-items:center;justify-content:center;font-size:22px}\n",".tag{font-size:12px;color:var(--muted)}\n",".desc{font-size:13px;color:var(--muted)}\n",".controls{margin-top:8px}\n","select{width:100%;padding:10px;border-radius:8px;border:none;background:rgba(255,255,255,0.02);color:#e8f6ff}\n",".footer{font-size:12px;color:var(--muted);margin-top:auto}\n","\n","/* main */\n",".main{flex:1;display:flex;flex-direction:column;gap:14px}\n","header{background:var(--card);padding:18px;border-radius:12px}\n",".uploader{background:var(--card);padding:18px;border-radius:12px;display:flex;flex-direction:column;gap:12px}\n",".dropzone{border:2px dashed rgba(255,255,255,0.04);padding:18px;border-radius:10px;display:flex;align-items:center;justify-content:center;cursor:pointer}\n",".dropzone:hover{background:rgba(255,255,255,0.01)}\n",".dropzone input{display:none}\n",".buttons{display:flex;gap:8px}\n","button{padding:10px 14px;border-radius:10px;border:none;cursor:pointer}\n",".primary{background:linear-gradient(90deg,var(--accent),#6b7bff);color:#012;font-weight:700}\n",".ghost{background:transparent;border:1px solid rgba(255,255,255,0.04);color:var(--muted)}\n",".small{background:rgba(255,255,255,0.02);color:var(--muted)}\n",".progress{height:10px;background:rgba(255,255,255,0.03);border-radius:8px;overflow:hidden;position:relative}\n",".bar{height:100%;width:0%;background:linear-gradient(90deg,var(--accent),#6b7bff);transition:width 400ms}\n",".ptext{position:absolute;right:10px;top:-22px;color:var(--muted);font-size:12px}\n","\n","/* results */\n",".results{margin-top:6px}\n",".result-card{background:linear-gradient(180deg,rgba(255,255,255,0.02),rgba(255,255,255,0.01));padding:12px;border-radius:12px}\n",".grid{display:grid;grid-template-columns:1fr 1fr;gap:12px}\n",".panel{background:rgba(0,0,0,0.35);padding:12px;border-radius:8px;min-height:120px;overflow:auto}\n",".panel.full{grid-column:1 / -1}\n",".hidden{display:none}\n","pre{white-space:pre-wrap;color:#e6f7ff;font-size:13px}\n","'''\n","\n","script_js = r'''\n","const dropzone = document.getElementById(\"dropzone\");\n","const fileInput = document.getElementById(\"fileInput\");\n","const analyzeBtn = document.getElementById(\"analyzeBtn\");\n","const clearBtn = document.getElementById(\"clearBtn\");\n","const downloadBtn = document.getElementById(\"downloadBtn\");\n","const bar = document.getElementById(\"bar\");\n","const ptext = document.getElementById(\"ptext\");\n","const results = document.getElementById(\"results\");\n","const simplified = document.getElementById(\"simplified\");\n","const ner = document.getElementById(\"ner\");\n","const clauses = document.getElementById(\"clauses\");\n","const classification = document.getElementById(\"classification\");\n","const featureSelect = document.getElementById(\"feature\");\n","\n","// dropzone UX\n","dropzone.addEventListener(\"click\", ()=> fileInput.click());\n","dropzone.addEventListener(\"dragover\", (e)=>{ e.preventDefault(); dropzone.style.borderColor = \"#6ee6ff\"; });\n","dropzone.addEventListener(\"dragleave\", (e)=>{ e.preventDefault(); dropzone.style.borderColor = \"\"; });\n","dropzone.addEventListener(\"drop\", (e)=>{ e.preventDefault(); fileInput.files = e.dataTransfer.files; });\n","\n","// clear\n","clearBtn.addEventListener(\"click\", ()=>{\n","  fileInput.value = \"\";\n","  results.classList.add(\"hidden\");\n","  bar.style.width = \"0%\";\n","  ptext.textContent = \"Waiting\";\n","  simplified.textContent = \"\"; ner.textContent=\"\"; clauses.textContent=\"\"; classification.textContent=\"\";\n","});\n","\n","// analyze\n","analyzeBtn.addEventListener(\"click\", async ()=>{\n","  if(!fileInput.files.length){ alert(\"Please select a file.\"); return; }\n","  const f = fileInput.files[0];\n","  const feature = featureSelect.value;\n","  const fd = new FormData(); fd.append(\"file\", f); fd.append(\"feature\", feature);\n","  bar.style.width = \"12%\"; ptext.textContent = \"Uploading...\";\n","  try{\n","    const resp = await fetch(\"/analyze\", { method: \"POST\", body: fd });\n","    bar.style.width = \"50%\"; ptext.textContent = \"Analyzing...\";\n","    if(!resp.ok){\n","      const err = await resp.json().catch(()=>({error:\"Server error\"}));\n","      alert(err.error || \"Server error\");\n","      bar.style.width = \"0%\"; ptext.textContent=\"Error\";\n","      return;\n","    }\n","    const data = await resp.json();\n","    bar.style.width = \"100%\"; ptext.textContent = \"Done\";\n","\n","    // adapt to feature\n","    if(feature === \"simplify\"){\n","      simplified.textContent = data.result || data.result_raw || data.analysis || JSON.stringify(data);\n","      results.classList.remove(\"hidden\");\n","      ner.textContent=\"\"; clauses.textContent=\"\"; classification.textContent=\"\";\n","    } else if(feature === \"ner\"){\n","      ner.textContent = data.result_raw || data.analysis || JSON.stringify(data);\n","      results.classList.remove(\"hidden\");\n","      simplified.textContent=\"\"; clauses.textContent=\"\"; classification.textContent=\"\";\n","    } else if(feature === \"clauses\"){\n","      clauses.textContent = data.result_raw || data.analysis || JSON.stringify(data);\n","      results.classList.remove(\"hidden\");\n","      simplified.textContent=\"\"; ner.textContent=\"\"; classification.textContent=\"\";\n","    } else if(feature === \"classify\"){\n","      classification.textContent = data.result_raw || data.analysis || JSON.stringify(data);\n","      results.classList.remove(\"hidden\");\n","      simplified.textContent=\"\"; ner.textContent=\"\"; clauses.textContent=\"\";\n","    } else {\n","      // all\n","      const analysis = data.analysis || data;\n","      simplified.textContent = analysis.simplified || JSON.stringify(analysis.simplified || \"\");\n","      ner.textContent = analysis.ner_raw || JSON.stringify(analysis.ner_raw || \"\");\n","      clauses.textContent = analysis.clauses_raw || JSON.stringify(analysis.clauses_raw || \"\");\n","      classification.textContent = analysis.classification_raw || JSON.stringify(analysis.classification_raw || \"\");\n","      results.classList.remove(\"hidden\");\n","    }\n","  }catch(e){\n","    alert(\"Request failed: \"+e.message);\n","    bar.style.width=\"0%\"; ptext.textContent=\"Failed\";\n","  }\n","});\n","\n","// download report (collect displayed panels)\n","downloadBtn.addEventListener(\"click\", async ()=>{\n","  const analysis = {\n","    \"Simplified\": simplified.textContent,\n","    \"NER\": ner.textContent,\n","    \"Clauses\": clauses.textContent,\n","    \"Classification\": classification.textContent\n","  };\n","  try{\n","    const res = await fetch(\"/download\", { method:\"POST\", headers:{\"Content-Type\":\"application/json\"}, body: JSON.stringify({analysis, filename:\"ClauseWise_Report.txt\"})});\n","    if(res.ok){\n","      const blob = await res.blob(); const url = URL.createObjectURL(blob); const a=document.createElement(\"a\"); a.href=url; a.download=\"ClauseWise_Report.txt\"; document.body.appendChild(a); a.click(); a.remove(); URL.revokeObjectURL(url);\n","    } else alert(\"Download failed\");\n","  }catch(e){ alert(\"Download failed: \"+e.message); }\n","});\n","'''\n","\n","# Write frontend files\n","with open(\"templates/index.html\",\"w\",encoding=\"utf-8\") as f: f.write(index_html)\n","with open(\"static/styles.css\",\"w\",encoding=\"utf-8\") as f: f.write(styles_css)\n","with open(\"static/script.js\",\"w\",encoding=\"utf-8\") as f: f.write(script_js)\n","\n","# ---------------- Start server (ngrok)\n","ngrok.set_auth_token(NGROK_TOKEN)\n","public_url = ngrok.connect(5000).public_url\n","print(\"üöÄ ClauseWise running at:\", public_url)\n","# Run flask app (blocking)\n","app.run(host=\"0.0.0.0\", port=5000)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dtZZZPLI1Xnf","executionInfo":{"status":"ok","timestamp":1756538379378,"user_tz":-330,"elapsed":194821,"user":{"displayName":"Karthika M","userId":"10246263488576393864"}},"outputId":"e1886d1b-a07d-42bd-a123-39748f64e4a6"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ ClauseWise running at: https://0bb51266ffde.ngrok-free.app\n"," * Serving Flask app '__main__'\n"," * Debug mode: off\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n"," * Running on all addresses (0.0.0.0)\n"," * Running on http://127.0.0.1:5000\n"," * Running on http://172.28.0.12:5000\n","INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n","INFO:werkzeug:127.0.0.1 - - [30/Aug/2025 07:16:42] \"GET / HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [30/Aug/2025 07:16:43] \"GET /static/styles.css HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [30/Aug/2025 07:16:43] \"GET /static/script.js HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [30/Aug/2025 07:16:44] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n","INFO:werkzeug:127.0.0.1 - - [30/Aug/2025 07:17:53] \"POST /analyze HTTP/1.1\" 200 -\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"5rabeU9t1Yts"},"execution_count":null,"outputs":[]}]}